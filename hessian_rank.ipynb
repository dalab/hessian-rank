{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank of Neural Hessians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code illustrates the results of the paper **Analytic Insights into Structure and Rank of Neural Network Hessian Maps** by calculating the rank of several Hessians and comparing them to the derived upper-bounds. The code is accompanied by the relevant formulas of the paper but of course is by no means an adequate substitution but is meant to provide context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import *\n",
    "from jax.config import config\n",
    "from jax.experimental.stax import softmax, logsoftmax\n",
    "from initializers import get_init\n",
    "\n",
    "from data import get_dataset\n",
    "from hessians import outer_prod, loss_hessian\n",
    "from architectures import fully_connected\n",
    "\n",
    "from dataloader import DatasetTorch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High numerical precision is essential to the calculations, thus all tensors will be of type **float64**. Using float32 leads to instabilities and indeed produces wrong results! Moreover, we found that float64 is vital to all calculations, not just the final rank computation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update(\"jax_enable_x64\", True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following setting. We have a neural network  $f_{{\\boldsymbol{\\theta}}}: \\mathbb{R}^{d} \\xrightarrow{} \\mathbb{R}^{k}$ with parameters $\\boldsymbol{\\theta} \\in \\mathbb{R}^{p}$, some input data $\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_n \\in \\mathbb{R}^{d}$ and targets $\\boldsymbol{y}_1, \\dots, \\boldsymbol{y}_n \\in \\mathbb{R}^{k}$.  Moreover we have some loss function $\\mathcal{L}(\\boldsymbol{\\theta})$ that measures how well we predict $\\boldsymbol{y}$. Here we will focus on the squared loss, \n",
    "$$\\mathcal{L}(\\boldsymbol{\\theta}) = \\sum_{i=1}^n||f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_i)-\\boldsymbol{y}_i{||}_2^2$$\n",
    " We want to calculate the Hessian of the loss with respect to the parameters $\\boldsymbol{\\theta}$, i.e\n",
    "$$\\boldsymbol{H}_{\\mathcal{L}} = \\frac{\\partial^2}{\\partial \\boldsymbol{\\theta} \\partial \\boldsymbol{\\theta}}\\mathcal{L}(\\boldsymbol{\\theta})$$\n",
    "Let us first setup the hyperparameters, namely the sample size $n$ of our problem, the dimensionality $d$ of our inputs and the number of classes $k$, i.e. the dimensionality of $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "n_train = 50                                                      # Sample size\n",
    "dim = 25                                                          # Dimension of data\n",
    "widths = [5, 10]                                                  # Width of the network, excluding last layer\n",
    "classes = 10                                                      # Number of classes\n",
    "bs = 10\n",
    "\n",
    "all_widths = [dim] + widths + [classes]\n",
    "p = sum([all_widths[i] * all_widths[i + 1] for i in range(len(all_widths)-1)])   \n",
    "\n",
    "# Initialize seed                           \n",
    "key = random.PRNGKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's specify the data distribution, we can choose from using 'MNIST', 'FashionMNIST' or 'CIFAR10'. We need to calculate the covariance matrix \n",
    "$$\\boldsymbol{\\Sigma} = \\sum_{i=1}^n \\boldsymbol{x}_i \\boldsymbol{x}_i^T$$\n",
    "and its rank $r = \\text{rank}(\\boldsymbol{\\Sigma})$, as $r$ will enter in our formulas for the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Define the data, we will choose down-scaled MNIST \n",
    "\n",
    "data = get_dataset('CIFAR', n_train=n_train, n_test=1, dim=dim, classes=classes)\n",
    "\n",
    "# Define a train loader so that we can batch the Hessian calculation\n",
    "train_loader = DataLoader(DatasetTorch(data.x_train, data.y_train), batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we fix our neural network model, which will be a fully-connected linear network of the following form:\n",
    "$$f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}) = \\boldsymbol{W}_L \\dots \\boldsymbol{W}_1 x$$\n",
    "where $\\boldsymbol{W}_i \\in R^{m_{i-1} \\times m_{i}}$ with $m_0 = d$ and $m_L = k$. For simplicity we will ignore biases, but if you're interested, check out the paper for the corresponding formulas with bias! Our theorems hold for a variety of initialization schemes, here you can choose from either 'glorot' (scaled Gaussian initialization), 'uniform' (scaled uniform initialization) or 'orthogonal' (sampling according to the Haar measure from the space of orthogonal matrices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose initialization\n",
    "init = get_init('glorot')                   \n",
    "# Define linear neural network architecture\n",
    "init_fn, apply_fn = fully_connected(units=widths, classes=classes, activation='linear', init=init)\n",
    "\n",
    "# Initialize the parameters\n",
    "_, params = init_fn(key, (-1, dim))\n",
    "\n",
    "# Make sure parameters are double precision\n",
    "params = [jnp.double(param) for param in params]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said above, here we focus on the mean-squared error \n",
    "$$\\mathcal{L}(\\boldsymbol{\\theta}) = \\sum_{i=1}^n||\\boldsymbol{y}_i-f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_i){||}_2^2$$\n",
    "Our theorems however also extend to the case of many other losses. For instance, use 'cross' for the cross entropy loss or 'cosh' for the cosh loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_name = 'mse'\n",
    "\n",
    "if loss_name == 'mse':\n",
    "    cross = False\n",
    "    \n",
    "    def loss(preds, targets):\n",
    "        return 1/2 * jnp.sum((preds - targets)**2)\n",
    "    \n",
    "    \n",
    "    def loss_params(params, inputs, targets):\n",
    "        preds = apply_fn(params, inputs)\n",
    "    \n",
    "        return 1/2 * jnp.sum((preds - targets)**2)\n",
    "\n",
    "if loss_name == 'cross':\n",
    "    cross = True\n",
    "    \n",
    "    def loss(preds, targets):\n",
    "        return -jnp.sum(logsoftmax(preds) * targets)\n",
    "\n",
    "\n",
    "    def loss_params(params, inputs, targets):\n",
    "        preds = apply_fn(params, inputs)\n",
    "\n",
    "        return -jnp.sum(logsoftmax(preds) * targets)\n",
    "    \n",
    "if loss_name == 'cosh':\n",
    "    cross = False\n",
    "    \n",
    "    def loss(preds, targets):\n",
    "        return jnp.sum(jnp.log(jnp.cosh(preds - targets)))\n",
    "\n",
    "    def loss_params(params, inputs, targets):\n",
    "        preds = apply_fn(params, inputs)\n",
    "\n",
    "        return loss(preds, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we come to the Hessian. We proceed to split into a functional part and a part that consists of the outer product of gradients, i.e.\n",
    "$$\\boldsymbol{H}_{\\mathcal{L}} = \\boldsymbol{H}_f + \\boldsymbol{H}_o$$\n",
    "where, for squared-loss, it holds that \n",
    "$$\\boldsymbol{H}_f = \\sum_{i=1}^n\\sum_{l=1}^k(y_{il}-f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_i))\\frac{\\partial^2 f_l(\\boldsymbol{x}_i)}{\\partial \\boldsymbol{\\theta} \\partial \\boldsymbol{\\theta}}$$\n",
    "and for the outer-gradient term we have\n",
    "$$\\boldsymbol{H}_o = \\sum_{i=1}^n\\sum_{l=1}^k \\frac{\\partial f_k(\\boldsymbol{x}_i)}{\\partial \\boldsymbol{\\theta}}\\left(\\frac{\\partial f_k(\\boldsymbol{x}_i)}{\\partial \\boldsymbol{\\theta}}\\right)^T$$\n",
    "For the formulas for more general loss functions, please check out our paper! Due to the structure of Jax, it is simpler (and more memory-efficient) to calculate the loss hessian and the outer product of gradients. Hence we will calculate the functional Hessian as \n",
    "$$\\boldsymbol{H}_f = \\boldsymbol{H}_{\\mathcal{L}} - \\boldsymbol{H}_o$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_L, H_outer = jnp.zeros(shape=(p, p)), jnp.zeros(shape=(p, p))\n",
    "cov = jnp.zeros(shape=(dim, dim))\n",
    "\n",
    "for batch_input, batch_label in train_loader:\n",
    "    batch_input, batch_label = (batch_input.numpy(), batch_label.numpy())\n",
    "    # Calculate the covariance\n",
    "    cov += batch_input.T @ batch_input\n",
    "    # Calculate loss hessian\n",
    "    H_L += loss_hessian(loss_params, params, batch_input, batch_label)\n",
    "    # Calculate the outer gradient product\n",
    "    H_outer += outer_prod(loss, apply_fn, params, batch_input, batch_label, cross=cross)\n",
    "\n",
    "# To save time we calculate the functional Hessian as the difference\n",
    "H_F = H_L - H_outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_cov = jnp.linalg.matrix_rank(cov)\n",
    "rank_L = jnp.linalg.matrix_rank(H_L)\n",
    "rank_outer = jnp.linalg.matrix_rank(H_outer)\n",
    "rank_F = jnp.linalg.matrix_rank(H_F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate our upper bounds on the Hessian rank, introduced in our paper and compare them with the numerical results. Our predictions, look as follows\n",
    "\\begin{align}\n",
    "\\text{rank}(\\boldsymbol{H}_o) &\\leq q(r + k - q) \\\\\n",
    "\\text{rank}(\\boldsymbol{H}_f) &\\leq 2q \\sum_{l=1}^L m_l + 2qs -Lq^2 \\\\\n",
    "\\text{rank}(\\boldsymbol{H}_L) &\\leq 2q \\sum_{l=1}^L m_l -Lq^2 + q(r+k)\n",
    "\\end{align}\n",
    "where $q=\\text{min}(r, k, m_1, \\dots m_L)$ and $s = \\text{min}(r, k)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of Functional Hessian is 175 and the prediction is 175\n",
      "Rank of Gradient Outer Product is 150 and the prediction is 150\n",
      "Rank of Loss Hessian is 250 and the prediction is 250\n"
     ]
    }
   ],
   "source": [
    "if loss_name == 'cross':\n",
    "    classes = classes - 1\n",
    "    \n",
    "s = jnp.min(jnp.array([rank_cov, classes]))\n",
    "q = jnp.min(jnp.array([rank_cov, classes] + widths))\n",
    "\n",
    "pred_F = 2 * q * sum(widths) + 2 * q * s - (len(widths)+1) * q**2\n",
    "pred_outer = (rank_cov + classes - q) * q\n",
    "pred_L = pred_F + pred_outer + q * (q - 2 * s)\n",
    "\n",
    "print('Rank of Functional Hessian is ' + str(rank_F) + ' and the prediction is ' + str(pred_F))\n",
    "\n",
    "print('Rank of Gradient Outer Product is ' + str(rank_outer) + ' and the prediction is ' + str(pred_outer))\n",
    "\n",
    "print('Rank of Loss Hessian is ' + str(rank_L) + ' and the prediction is ' + str(pred_L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1.0,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1.0,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
